= About the Capstone design project

* The purpose of this project is to help new parents understand their babies' pose-based real-time bodily motions (such as arching back, headbanging, kicking legs, wiping eyes, stretching, and sucking fingers).

* We used the "MoveNet" Model for the pre-processing step.

== The dataset

* The dataset consists of 145 video clips for six different activity classes: arching back, sucking fingers, stretching, kicking legs, rubbing eye and headbanging.

** Crawled from video sharing platforms such as YouTube, TikTok, etc.

** The dataset used in this project can be found at the following link: https://github.com/meyurtsever/BabyPose[BabyPose, role=external,window=_blank]

* These video clips are incorporated into the HPE (Human Pose Estimation) models: https://github.com/CMU-Perceptual-Computing-Lab/openpose[OpenPose, role=external,window=_blank].

** These coordinate outputs of body parts are normalized, preprocessed, and used in the deep learning model to predict activities.

* Finally, pre-processed HPE outputs are put together to create train and test datasets (See https://github.com/DayenaJeong/capstone2_SEDA/blob/main/openpose/json_output_openpose/all_combined_jsons.txt[-> (all_combined_jsons.txt),role=external,window=_blank]).

= Installation

* Clone the project:
[source,bash]
----
git clone https://github.com/DayenaJeong/capstone2_SEDA.git
----
