= About the Capstone design project

* The purpose of this project is to help new parents understand their babies' pose-based real-time bodily motions (such as arching back, headbanging, kicking legs, wiping eyes, stretching, and sucking fingers).

* We used the "OpenPose" Model for the pre-processing step.

== The dataset

* The dataset consists of 145 video clips for six different activity classes: arching back, sucking fingers, stretching, kicking legs, rubbing eye and headbanging.

** Crawled from video sharing platforms such as YouTube, TikTok, etc.

** The dataset used in this project can be found at the following link: https://github.com/meyurtsever/BabyPose[BabyPose, role=external,window=_blank]

* These video clips are incorporated into the HPE (Human Pose Estimation) models: https://github.com/CMU-Perceptual-Computing-Lab/openpose[OpenPose, role=external,window=_blank].

** These coordinate outputs of body parts are normalized, preprocessed, and used in the deep learning model to predict activities.

== Data pre-processing

* JSON/CSV outputs of HPE models have excess information for our application.

* For pre-processing, extracting only x and y coordinates of detected body parts is necessary for our deep learning model.

* Pre-processing step for:

** OpenPose https://github.com/DayenaJeong/capstone2_SEDA/blob/main/openpose/1_json_to_txt_openpose.ipynb[-> (1_json_to_txt_openpose.ipynb),role=external,window=_blank]

* Finally, pre-processed HPE outputs are put together to create train and test datasets (See merge_all_txt notebooks).

== All of the steps for the OpenPose HPE

= Installation

* We recommend https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html[installing Anaconda or Miniconda,role=external,window=_blank] for switching between virtual environments.

* Clone the project:
[source,bash]
----
git clone https://github.com/DayenaJeong/capstone2_SEDA.git
----

== YOLOv4-tiny

* We use yolov4-tiny for tracking babies in the demo. Due to its size, yolov4-tiny is uploaded to Google Drive.
** You can manually download the YOLO model https://drive.google.com/uc?id=1JVNpQvsg6oi5fzhRQuKDjQrXTGMRfpT-[from here.,role=external,window=_blank] +
** YOLOv4-tiny is a fork https://github.com/theAIGuysCode/yolov4-deepsort[from this repository.,role=external,window=_blank]

* Install https://github.com/wkentaro/gdown[gdown,role=external,window=_blank]  (for downloading large files from Google Drive):
[source,bash]
----
pip install gdown
----

In the root directory of BabyPose: +
[source,bash]
----
gdown 1JVNpQvsg6oi5fzhRQuKDjQrXTGMRfpT- --output yolov4-deepsort.zip && tar zxvf yolov4-deepsort.zip && rm yolov4-deepsort.zip
----
The above code gets the YOLOv4-tiny model from Google Drive, extracts the zip, and removes the zip file after extracting. +

You can use `Tex` or `MathML` languages for

== Install OpenPose HPE

* Create virtual environment for OpenPose
[source,bash]
----
conda create --name openpose
conda activate openpose
----
* Install OpenPose https://github.com/CMU-Perceptual-Computing-Lab/openpose#installation[from their guide,role=external,window=_blank]  (GPU is recommended).
* You'll need TensorFlow, OpenCV, and keras for the inference.
[source,bash]
----
pip install Keras==2.4.3
pip install Keras-Preprocessing==1.1.2
pip install tensorflow==2.3.0
pip install opencv-python==4.4.0.44
conda activate openpose
----

=== Inference with OpenPose

* Place demo videos to `demo` folder. +
* In the root directory of BabyPose:

[source,bash]
----
cd openpose
python predict_activity_openpose.py -i demo/baby_stretching.mp4
----
